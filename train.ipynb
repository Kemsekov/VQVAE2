{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Kemsekov/kemsekov_torch\n",
    "# !cd kemsekov_torch && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from kemsekov_torch.train import split_dataset\n",
    "\n",
    "# folder with images that will be used to train vqvae\n",
    "images_folder = 'train_images_folder'\n",
    "\n",
    "\n",
    "TRAIN_IMAGE_SIZE=(128,128)\n",
    "\n",
    "# simple image augmentations\n",
    "interpolation = T.InterpolationMode.NEAREST\n",
    "tr = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Lambda(lambda x: x[:3]),\n",
    "    T.Resize(int(1.12*TRAIN_IMAGE_SIZE[0])),\n",
    "    T.RandomCrop(TRAIN_IMAGE_SIZE),\n",
    "    T.ColorJitter(0.5,0.5,0.5),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "])\n",
    "dataset = ImageFolder(images_folder,transform=tr)\n",
    "\n",
    "# split dataset into train and eval\n",
    "train_dataset,test_dataset,train_loader, test_loader = split_dataset(\n",
    "    dataset,\n",
    "    test_size=0.05,\n",
    "    num_workers=16,\n",
    "    batch_size=16\n",
    ")\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "# Set up a 4x4 grid for displaying images\n",
    "fig, axes = plt.subplots(4, 4, figsize=(5, 5))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        index = randint(0, len(dataset) - 1)       # Random index from dataset\n",
    "        sample = dataset[index]                    # Select a random sample\n",
    "        image, label = sample[0], sample[1]        # Separate image and label\n",
    "        ax = axes[i, j]                            # Select subplot position\n",
    "        # Display image on the selected subplot\n",
    "        ax.imshow(T.ToPILImage()(image))\n",
    "        ax.axis(\"off\")                             # Hide axes for clean view\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def vqvae2_loss(x,x_rec,z,z_q,beta=0.25):\n",
    "    \"\"\"\n",
    "    Computes loss for vqvae2 results.\n",
    "    \n",
    "    x: original x\n",
    "    x_rec: reconstruction\n",
    "    z: list of embeddings from encoder\n",
    "    z_q: list of quantized embeddings\n",
    "    beta: how fast to update encoder outputs z relative to reconstruction loss term\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_ = lambda x,y : ((x-y)**2).mean()\n",
    "    \n",
    "    # general mse reconstruction loss\n",
    "    # TODO: add perceptual loss\n",
    "    rec_loss = loss_(x,x_rec)\n",
    "    \n",
    "    commitment_loss = 0\n",
    "    # commitment loss\n",
    "    for z_,z_q_ in zip(z,z_q):\n",
    "        commitment_loss += loss_(z_,z_q_.detach())/len(z)\n",
    "    commitment_loss/=len(z)\n",
    "    \n",
    "    return rec_loss+beta*commitment_loss\n",
    "\n",
    "# r2 score that will be used in metrics\n",
    "def r2(x,y):\n",
    "    if isinstance(x,list):\n",
    "        return sum([r2(a,b) for a,b in zip(x,y)])/len(x)\n",
    "    return r2_score(x.detach().cpu().flatten(),y.detach().cpu().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.vqvae.vqvae2 import VQVAE2Scale3\n",
    "codebook_size=[512,512,512]\n",
    "\n",
    "# 3 - levels hierarchical vqvae\n",
    "vqvae = VQVAE2Scale3(\n",
    "    in_channels=3,\n",
    "    latent_dim=16,\n",
    "    num_residual_layers=5,\n",
    "    embedding_dim=96,\n",
    "    codebook_size=[512,512,512],\n",
    "    compression_ratio=4,\n",
    "    dimensions=2, # we work with 2-dim images\n",
    ").eval()\n",
    "\n",
    "# see output shapes\n",
    "s = dataset[0][0][None,:]\n",
    "[[b.shape for b in v] if isinstance(v,list) else v.shape for v in vqvae(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.train import train\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # to ignore pil rbg channels warnings\n",
    "\n",
    "def vqvae2_compute_loss_and_metric(model,batch):\n",
    "    model.requires_grad_(True)\n",
    "    x = batch[0]\n",
    "    \n",
    "    # feed input image to model\n",
    "    # get reconstruction,latents and quantized latents\n",
    "    x_rec,z,zq,_ = model(x)\n",
    "    x_rec=x_rec.sigmoid()\n",
    "\n",
    "    loss = vqvae2_loss(x,x_rec,z,zq)\n",
    "\n",
    "    # get information about codebooks usage\n",
    "    usage_bottom = model.quantizer_bottom.get_codebook_usage()\n",
    "    usage_mid = model.quantizer_mid.get_codebook_usage()\n",
    "    usage_top = model.quantizer_top.get_codebook_usage()\n",
    "\n",
    "    return loss,{\n",
    "        \"rec_r2\":r2(x,x_rec),\n",
    "        \"z_bottom_r2\":r2(z[0],zq[0]),\n",
    "        \"z_mid_r2\":r2(z[1],zq[1]),\n",
    "        \"z_top_r2\":r2(z[2],zq[2]),\n",
    "        \"usage_bottom\":usage_bottom,\n",
    "        \"usage_mid\":usage_mid,\n",
    "        'usage_top':usage_top,\n",
    "    }\n",
    "\n",
    "optim = torch.optim.AdamW(vqvae.parameters())\n",
    "scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optim,T_max=len(train_loader))\n",
    "# where to save training checkpoints\n",
    "vqvae2_path = 'runs/vqvae2-16-4x'\n",
    "\n",
    "vqvae = train(\n",
    "    model=vqvae,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    compute_loss_and_metric=vqvae2_compute_loss_and_metric,\n",
    "    optimizer=optim,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=1000,\n",
    "    save_on_metric_improve=['rec_r2'],\n",
    "    save_results_dir=vqvae2_path,\n",
    "    load_checkpoint_dir=os.path.join(vqvae2_path,'last'),\n",
    "    accelerate_args={\n",
    "        'mixed_precision':'bf16',\n",
    "        'dynamo_backend':'inductor'\n",
    "    },\n",
    "    gradient_clipping_max_norm=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
